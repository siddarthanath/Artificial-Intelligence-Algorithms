{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate a step-by-step understanding of a Feed-Forward Neural Network (FFNN). In particular, we will be focussing on the implementation of a FFNN in **PyTorch** for classification and regression tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition (Visual)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/logistic_regression_comparison_nn5.png\" width=\"750\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition (Theoretical)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A feed-forward neural network (FFNN) is an artificial neural network (ANN), wherein connections between the nodes do not form a cycle (but instead move *forward* through the network).\n",
    "- A FFNN is comprised of:\n",
    "1. Input Layer\n",
    "2. Hidden Layers\n",
    "3. Output Layer\n",
    "- A FFNN can be shallow/deep, as well as narrow/wide. These properties depend on: \n",
    "1. Number of hidden layers\n",
    "2. Number of hidden units in the layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The input layer is the initial data being feed through the network.\n",
    "- Depending on the Python library being used and type of data, this may be 2D or 3D.\n",
    "- A linear transformation is applied to the input data in the form of $Y = WX + b$, where $W$ are the weights of the neural network and $b$ is the bias of the neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The hidden layers lie in between the input and output layer.\n",
    "- Once we apply the linear transformation to the input data, we obtain the form $a_{1} = WX+b$ (where 1 indicates that we are entering the first hidden layer) - $a_{1}$ is known as the pre-activation hidden layer. After, we apply a non-linear activation (this could be sigmoid, tanh etc...) and we obtain $h_{1} = \\sigma\\left(a_{1}\\right)$ - $h_{1}$ is known as the post-activation hidden layer (in the first hidden layer).\n",
    "- We then repeat this process (applying linear and non-linear activations) till we arrive at the output layer.\n",
    "- Ths encodes the original data such that the output layer can be interpreted more easily and so that we can learn/capture complex relationships within the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cnce the final hidden layer has been linearly transformed, we enter the output layer (the final layer of the network).  \n",
    "- A suitable non-linear activation function must be applied here (which will depend on the machine learning task). Popular functions exist such as softmax, crossentropy etc..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2017/10/mlp-diagram.jpg\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A logistic regression is simply a linear regression ($y = WX+b$), with a non-linear activation applied to it i.e. sigmoid. This means that the logistic regression is simply a 1 layer FFNN.\n",
    "- If all the non-linear activation functions were the identity function, the FFNN would simply be a special case of linear regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using ```torch```, try not to mix ```numpy``` into any calculation or dataset conversions. Additionally, check if you have access to ```GPU``` via ```torch.cuda.is_available()```. To use GPU, apply ```torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")```."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this example we will be looking at famous MNIST dataset (for multiclass classification).\n",
    "- This dataset is readily available via the *torch* library - in some cases, you may need to load/create the data yourselves.\n",
    "- If so, it can be useful to store the data similarly to the way that *torch* does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "# Test set\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Process Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depending on if you are loading the data or creating the data yourself, you may need to clean the data so that the model can use it. This is perhaps the most trickiest part throughout a Data Science/Machine Learning pipeline.\n",
    "- In this case, it is good to make *batches* of the dataset so that we can manipulate the way the data is being trained e.g. allow shuffling when training per epoch etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide batch size in each portion of the data loader object\n",
    "batch_size = 128\n",
    "# Decide the sizes of the datasets\n",
    "train_size = int(0.8 * len(train_dataset)) \n",
    "val_size = len(train_dataset) - train_size \n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise data\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "num_images = 16\n",
    "# Create plots\n",
    "fig, axs = plt.subplots(4, 4, figsize=(8, 8))\n",
    "axs = axs.flatten()\n",
    "for i in range(num_images):\n",
    "    # Convert to 1-D tensor\n",
    "    img = images[i].squeeze()\n",
    "    # Images have 1 channel i.e. greyscale so need to use \"gray\" mapping\n",
    "    axs[i].imshow(img, cmap='gray')\n",
    "    axs[i].title.set_text('Label: ' + str(labels[i].item()))\n",
    "    axs[i].axis('off')  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ```squeeze```, we convert each image in a batch to a 1D tensor of i.e. originally we would have ```[128, 1, 28, 28]```, but after squeeze we have ```[128, 28, 28]```. Note that the $128$ refers to the *batch size*, the $\\left[28, 28\\right]$ refers to the *image dimension* and the $1$ refers to the tensor structure. Also, if we look at ```len(dataiter)```, we obtain $468$ - this number refers to the number image collections present (where in each collectin, we select a batch of $128$ images from the entire dataset). Hence, the number of total images in the entire MNIST dataset can be calculated as ```(len(iter(train_loader)) + len(iter(test_loader))) x 128```, which gives us $70016$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once the data is ready, we can now look to choose what kind of model we want to create.\n",
    "- For this notebook, we will implement a FFNN. Other models exist e.g. Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 Layer FFNN\n",
    "class FFNN_Long(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        # Allows for multiple inheritance\n",
    "        super(FFNN_Long, self).__init__()\n",
    "        # Linear Function (applied to input data) i.e. pre-activation first hidden layer.\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-Linear Function (applied to the pre-activation hidden layer) i.e. post-activation first hidden layer.\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (applied to the post-activation hidden layer) i.e. final pre-activation output layer.\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Pre-activation first hidden layer\n",
    "        out = self.fc1(x)\n",
    "        # Post-activation first hidden layer\n",
    "        out = self.relu(out)\n",
    "        # Final pre-activation output layer\n",
    "        out = self.fc2(out)\n",
    "        # Final post-activation output layer (NOTE: This might not be needed if the loss function incorporates the calculation)\n",
    "        out = nn.LogSoftmax(out, dim = 1)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ```nn.LogSoftmax``` than ```nn.Softmax``` helps improve numerical stability.\n",
    "We can condense the architecture above by using ```nn.Sequential``` in the ```__init__``` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 Layer FFNN\n",
    "class FFNN_Short(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        # Allows for multiple inheritance\n",
    "        super(FFNN_Short, self).__init__()\n",
    "        # Create network\n",
    "        self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Create network\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that ```nn.LogSoftmax``` has been removed is because later on, we will be defining our loss as ```nn.CrossEntropyLoss```, which takes into account the ```nn.LogSoftmax``` calculation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Instantiate Model Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the architecture and data are in the correct form, we can being initialise our class.\n",
    "- Once this is complete, we can begin training.\n",
    "- We may run into some errors from time to time - this usually occurs because of incompatible dimensions or incorrect data types so be sure to fix these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arguments\n",
    "kwargs = {\"input_dim\": 28*28, \n",
    "          \"hidden_dim\": 100,\n",
    "          \"output_dim\": 10}\n",
    "# Create model\n",
    "model = FFNN_Short(**kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Instantiate Optimiser and Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to update our weights (to find the optimal ones) and track the model performance, we need to choose an optimiser and loss function.\n",
    "- The loss function is typically straightforward (as it depends on the machine learning task) however there are a plethora of optimisers you could use e.g. SGD, Adam etc... Usually this is a trial and error choice to find the *best* one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Instantiate optimiser\n",
    "opt = torch.optim.Adam(params=model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to recognise that ```model.parameters()``` are the parameters that we wish to update/optimise. If these are not correctly defined in the ```__init__```, then the network may not be fully functional."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are now ready to train the model.\n",
    "- Through training, it is also useful to record some metrics to see how the model is performing.\n",
    "- We use the loss of model to dictate potential *early stopping* of the model (although we could use other measures as well) i.e. if the model loss starts increasing after it has decreased, we are potentially overfitting and so should stop training and keep the model at that instance. This can be applied per ```epoch``` or at some multiple of ```n_iters```.\n",
    "- When evaluating the model, it is necessary to apply ```model.eval()``` along with ```torch.no_grad()``` and once this is complete, to apply ```model.train()``` after if the model architecture includes layers of ```dropout```, ```batch_normalisation``` etc...\n",
    "- The training and evaluation phase below is a generalised architecture; there are other variations of this which you may want to experiment with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of epochs\n",
    "n_iters = 15000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the early stopping parameters\n",
    "best_val_loss = float('inf') \n",
    "patience = 3\n",
    "epochs_no_improve = 0\n",
    "early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate through entire collection (per batch)\n",
    "    for images, labels in train_loader:\n",
    "        # Clear gradients with respect to parameters\n",
    "        opt.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(images.view(images.shape[0], -1))\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Parameter update\n",
    "        opt.step()\n",
    "    # Evaluate model (NOTE: In the evaluation phase, the model parameters do not need updating!)\n",
    "    with torch.no_grad():\n",
    "        # Initialise validation loss \n",
    "        val_loss = 0\n",
    "        # Initialise secondary metric\n",
    "        correct = 0\n",
    "        # Iterate through entire collection (per batch)\n",
    "        for images, labels in val_loader: \n",
    "            # Forward pass\n",
    "            outputs = model(images.view(images.shape[0], -1))\n",
    "            # Obtain predictons\n",
    "            _, preds = torch.max(outputs.data, dim=1)\n",
    "            # Total correct predictions\n",
    "            correct += (preds == labels).sum()\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # Accumulate loss\n",
    "            val_loss += loss.item()\n",
    "        # Record secondary metric\n",
    "        val_acc_score = 100 * correct / len(val_loader.dataset)\n",
    "        # Print loss and accuracy\n",
    "        print(f\"Epoch: {epoch} | Validation loss: {val_loss} | Validation accuracy: {val_acc_score}\")\n",
    "        # If the validation loss is at a new minimum, save the model\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        # If the validation loss is not improving for certain patience, stop training!\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                early_stop = True\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model has been saved\n",
    "try:\n",
    "    f = open('./best_model.pth', 'r') \n",
    "    model.load_state_dict(torch.load('best_model.pth')) \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Model not found!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the model has been trained and evaluated, we can test the model on unseen data.\n",
    "- This is a crucial part as it shows whether or not the model is generalisable i.e. has low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "correct = 0\n",
    "for images, labels in test_loader: \n",
    "    # Forward pass\n",
    "    outputs = model(images.view(images.shape[0], -1))\n",
    "    # Obtain predictons\n",
    "    _, preds = torch.max(outputs.data, dim=1)\n",
    "    # Total correct predictions\n",
    "    correct += (preds == labels).sum()\n",
    "# Record secondary metric\n",
    "test_acc_score = 100 * correct / len(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise predictions\n",
    "images, labels = next(iter(test_loader))\n",
    "# Predict labels for the test images\n",
    "with torch.no_grad():\n",
    "    outputs = model(images.view(images.shape[0], -1))\n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "# Plot the images with their predicted labels\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(images[i].view(28, 28), cmap='gray') \n",
    "    plt.title(f\"Predicted: {predicted[i]}, Actual: {labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that using accuracy may not be the best metric to use to quantify the model performance - there are other metrics we can use e.g. F-1, Precision etc... Additionally, it may be convenient to ```flatten``` the arrays within the dataset so that we do not have to do ```images.view(images.shape[0], -1)``` (which may require some data manipulation before storing it as a ```DataLoader```)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Remarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading this notebook. Note that there are other implementations of feed-forward neural networks (which I would advise you to take a look at to see any differences of similarities with this version).\n",
    "If there are any mistakes or things that need more clarity, feel free to respond and I will be happy to reply ðŸ˜Š.\n",
    "\n",
    "Â© *PolyNath 2023*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucl-bdl-2022",
   "language": "python",
   "name": "ucl-bdl-2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
