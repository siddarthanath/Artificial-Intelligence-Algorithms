{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "\n",
    "    \"\"\"\n",
    "    Initialise epsilon-greedy agent.\n",
    "    - This agent returns an action between 0 and 'number_of_arms'.\n",
    "    - It does so with probability `(1-epsilon)` it chooses the action with the highest estimated value, while\n",
    "    with probability `epsilon`, it samples an action uniformly at random.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, number_of_arms: int, epsilon=Union[float, callable]):\n",
    "        self.name = name\n",
    "        self._number_of_arms = number_of_arms\n",
    "        self._epsilon = epsilon\n",
    "        self.reset()\n",
    "\n",
    "    \"\"\"\n",
    "    Execute Epsilon-Greedy agent's next action and update Epsilon Greedy's action-state values.\n",
    "    \"\"\"\n",
    "    def step(self, previous_action: Optional[int], reward: float) -> int:\n",
    "        # Check if epsilon is scalar or callable\n",
    "        new_epsilon = self._epsilon if np.isscalar(self._epsilon) else self._epsilon(self.t)\n",
    "        # Check if previous action does not exist i.e. on first action\n",
    "        if previous_action != None:\n",
    "            # Update action count for previous action\n",
    "            self.N_t[previous_action] += 1\n",
    "            # Use iterative form of Q_t(a)\n",
    "            self.Q_t[previous_action] += (reward - self.Q_t[previous_action]) / self.N_t[previous_action]\n",
    "            # A_t(a) is the 'action' chosen at time step 't'\n",
    "            action = np.random.choice(np.where(self.Q_t == np.max(self.Q_t))[0]) if np.random.uniform() < 1 - new_epsilon else np.random.randint(0, self.Q_t.shape[0])\n",
    "        else:\n",
    "            # A_t(a) is the 'action' chosen at time step 't'\n",
    "            action = np.random.choice(np.where(self.Q_t == np.max(self.Q_t))[0]) if np.random.uniform() < 1 - new_epsilon else np.random.randint(0, self.Q_t.shape[0])\n",
    "        # Update time step counter\n",
    "        self.t += 1\n",
    "        # Update true rewards\n",
    "        self.rewards.append(reward)\n",
    "        # Update actions\n",
    "        self.actions.append(action)\n",
    "\n",
    "    \"\"\"\n",
    "    Reset Epsilon Greedy agent.\n",
    "    \"\"\"\n",
    "    def reset(self):\n",
    "        # Q_t(a) is the estimated value of action â€˜aâ€™ at time step â€˜tâ€™\n",
    "        self.Q_t = np.zeros(self._number_of_arms)\n",
    "        # N_t(a) is the number of times that action â€˜aâ€™ has been selected, prior to time â€˜tâ€™\n",
    "        self.N_t = np.zeros(self._number_of_arms)\n",
    "        # Set time step counter\n",
    "        self.t = 1\n",
    "        # Set true rewards\n",
    "        self.rewards = []\n",
    "        # Set actions\n",
    "        self.actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (518508592.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class UCB:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialise UCB agent. \n",
    "    - This agent returns an action between 0 and 'number_of_arms'.\n",
    "    - This agent uses uncertainty in the action-value estimates for balancing exploration and exploitation.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, number_of_arms: int, bonus_multiplier: float):\n",
    "        self._number_of_arms = number_of_arms\n",
    "        self._bonus_multiplier = bonus_multiplier\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    \"\"\"\n",
    "    Execute UCB agent's next action and update UCB's action-state values.\n",
    "    \"\"\"\n",
    "    def step(self, previous_action: Optional[int], reward: Union[float, int]) -> int:\n",
    "        # All actions must be selected at least once before UCB is applied\n",
    "        if np.any(self.N_t == 0):\n",
    "            # Select non-explored action\n",
    "            action = np.random.choice(np.where(self.N_t == 0)[0])\n",
    "            self.N_t[previous_action] += 1\n",
    "        else:\n",
    "            self.N_t[previous_action] += 1\n",
    "            # Use iterative form of Q_t(a)\n",
    "            self.Q_t[previous_action] += (1 / self.N_t[previous_action]) * (reward - self.Q_t[previous_action])\n",
    "            # Calculate expected reward values\n",
    "            reward_values = self.Q_t + self._bonus_multiplier * np.sqrt(np.log(self.t) / self.N_t)\n",
    "            reward_values[np.isnan(reward_values)] = -np.inf\n",
    "            # A_t(a) is the 'action' chosen at time step 't'\n",
    "            action = np.random.choice(np.where(reward_values == np.nanmax(reward_values))[0])\n",
    "        # Update time step counter\n",
    "        self.t += 1\n",
    "        # Update true rewards\n",
    "        self.rewards.append(reward)\n",
    "        # Update actions\n",
    "        self.actions.append(action)\n",
    "\n",
    "    \"\"\"\n",
    "    Reset UCB agent.\n",
    "    \"\"\"\n",
    "    def reset(self):\n",
    "        # Q_t(a) is the estimated value of action â€˜aâ€™ at time step â€˜tâ€™\n",
    "        self.Q_t = np.zeros(self._number_of_arms)\n",
    "        # N_t(a) is the number of times that action â€˜aâ€™ has been selected, prior to time â€˜tâ€™\n",
    "        self.N_t = np.zeros(self._number_of_arms)\n",
    "        # Set time step counter\n",
    "        self.t = 1\n",
    "        # Set true rewards\n",
    "        self.rewards = []\n",
    "        # Set actions\n",
    "        self.actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Greedy Algorithm\n",
    "greedy_alg = EpsilonGreedy(name=\"Greedy\", \n",
    "                           number_of_arms=3, \n",
    "                           epsilon=0)\n",
    "# Define Epsilon-Greedy Algorithm\n",
    "eps_greedy_alg = EpsilonGreedy(name=\"Epsilon-Greedy\", \n",
    "                           number_of_arms=3, \n",
    "                           epsilon=0.4)\n",
    "# Define UCB Algorithm\n",
    "ucb_alg = UCB(name=\"UCB\", \n",
    "              number_of_arms=3, \n",
    "              bonus_multiplier=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_armed_bandits(algs: List, reward_dist: Dict[int, Union[int, float]], start_action: int, num_iter: int):\n",
    "    # Execute remaining steps of algorithm\n",
    "    for j in range(num_iter):\n",
    "        # Loop over each algorithm\n",
    "        for i in range(len(algs)):\n",
    "            # Execute first step of algorithm\n",
    "            if j == 0:\n",
    "                algs[i].actions.append(start_action)\n",
    "                algs[i].rewards.append(reward_dist[start_action])\n",
    "            # Execute steps after the first step\n",
    "            else:\n",
    "                algs[i].step(previous_action=algs[i].actions[j-1], reward=reward_dist[algs[i].actions[j-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[2]\n",
      "[2]\n",
      "[0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m kwargs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39malgs\u001b[39m\u001b[39m\"\u001b[39m: [greedy_alg, eps_greedy_alg, ucb_alg],\n\u001b[1;32m      3\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mreward_dist\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m0\u001b[39m: np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(low\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, high\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m), \u001b[39m1\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m2\u001b[39m: np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(loc\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, scale\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m)},\n\u001b[1;32m      4\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mstart_action\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m),\n\u001b[1;32m      5\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnum_iter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m50\u001b[39m}\n\u001b[1;32m      6\u001b[0m \u001b[39m# NOTE: Since this is in place, the algorithms will get updated dynamically!\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m run_multi_armed_bandits(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mrun_multi_armed_bandits\u001b[0;34m(algs, reward_dist, start_action, num_iter)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(algs[i]\u001b[39m.\u001b[39mactions)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Execute steps after the first step\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     algs[i]\u001b[39m.\u001b[39mstep(previous_action\u001b[39m=\u001b[39malgs[i]\u001b[39m.\u001b[39;49mactions[j], reward\u001b[39m=\u001b[39mreward_dist[algs[i]\u001b[39m.\u001b[39mactions[j]])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Run algorithm\n",
    "kwargs = {\"algs\": [greedy_alg, eps_greedy_alg, ucb_alg],\n",
    "          \"reward_dist\": {0: np.random.uniform(low=-2, high=2), 1: 5, 2: np.random.normal(loc=10, scale=0.05)},\n",
    "          \"start_action\": np.random.randint(0, 3),\n",
    "          \"num_iter\": 50}\n",
    "# NOTE: Since this is in place, the algorithms will get updated dynamically!\n",
    "run_multi_armed_bandits(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative rewards\n",
    "plt.plot(np.cumsum(greedy_alg.rewards), label=\"Greedy\")\n",
    "plt.plot(np.cumsum(eps_greedy_alg.rewards), label=\"Epsilon Greedy\")\n",
    "plt.plot(np.cumsum(ucb_alg.rewards), label=\"UCB\")\n",
    "plt.legend()\n",
    "plt.title(\"Cumulative Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of pulled arms\n",
    "categories = [\"Arm 1\", \"Arm 2\", \"Arm 3\"]\n",
    "bar_width = 0.2 \n",
    "index = np.arange(len(categories)) \n",
    "# Create bar plots for each set of values\n",
    "plt.bar(index - bar_width, greedy_alg.N_t, bar_width, label='Greedy')\n",
    "plt.bar(index, eps_greedy_alg.N_t, bar_width, label='Epsilon Greedy')\n",
    "plt.bar(index + bar_width, ucb_alg.N_t, bar_width, label='UCB')\n",
    "# Customise the plot\n",
    "plt.xlabel('Arms')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(r'Number of times each arm is pulled - $N_{t}\\left(a\\right)$')\n",
    "plt.xticks(index, categories)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of pulled arms\n",
    "categories = [\"Arm 1\", \"Arm 2\", \"Arm 3\"]\n",
    "bar_width = 0.2 \n",
    "index = np.arange(len(categories)) \n",
    "# Create bar plots for each set of values\n",
    "plt.bar(index - bar_width, greedy_alg.Q_t, bar_width, label='Greedy')\n",
    "plt.bar(index, eps_greedy_alg.Q_t, bar_width, label='Epsilon Greedy')\n",
    "plt.bar(index + bar_width, ucb_alg.Q_t, bar_width, label='UCB')\n",
    "# Customise the plot\n",
    "plt.xlabel('Arms')\n",
    "plt.ylabel('Reward Estimate Value')\n",
    "plt.title(r'Approximate Reward Estimate Value Function - $Q_{t}\\left(a\\right)$')\n",
    "plt.xticks(index, categories)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What do you notice about $N_{t}\\left(a\\right)$ for the respective algorithms? Do any of them perform similarly or differently?\n",
    "2. What do you notice about $Q_{t}\\left(a\\right)$ for the respective algorithms? Do any of them perform similarly or differently?\n",
    "3. What happens if you change the $epsilon$ value in Epsilon-Greedy?\n",
    "4. What happens if you change the $multiplier$ value in UCB?\n",
    "5. What happens if you change the iteration length?\n",
    "6. What happens if you make sure that all arms are pulled, at least once first, and then continue the algorithm? Note that only UCB does this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading this notebook. Note that there are other implementations of recurrent neural networks (which I would advise you to take a look at to see any differences of similarities with this version).\n",
    "If there are any mistakes or things that need more clarity, feel free to respond and I will be happy to reply ðŸ˜Š.\n",
    "\n",
    "Â© *PolyNath 2023*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-2023",
   "language": "python",
   "name": "msc-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
