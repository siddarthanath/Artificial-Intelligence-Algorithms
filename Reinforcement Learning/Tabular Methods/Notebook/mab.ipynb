{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the setup below, we assume that we start at $t=0$ and thus there is no previous action or current reward. We also assume that all arms must be pulled at least once before executing the entire algorithm - this is known $warm\\_up$. Depending on the current needs, we can adapt the code - an alternative setup is considered in the `.py` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "\n",
    "    \"\"\"\n",
    "    Initialise epsilon-greedy agent.\n",
    "    - This agent returns an action between 0 and 'number_of_arms'.\n",
    "    - It does so with probability `(1-epsilon)` it chooses the action with the highest estimated value, while\n",
    "    with probability `epsilon`, it samples an action uniformly at random.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, number_of_arms: int, epsilon=Union[float, callable]):\n",
    "        self.name = name\n",
    "        self._number_of_arms = number_of_arms\n",
    "        self._epsilon = epsilon\n",
    "        self.reset()\n",
    "\n",
    "    \"\"\"\n",
    "    Execute Epsilon-Greedy agent's next action and update Epsilon Greedy's action-state values.\n",
    "    \"\"\"\n",
    "    def step(self, reward_dist: List[Union[float, int]]) -> int:\n",
    "        # All actions must be selected at least once before Epsilon-Greedy is applied\n",
    "        if np.any(self.N_t == 0):\n",
    "            # Select non-explored action\n",
    "            action = np.random.choice(np.where(self.N_t == 0)[0])\n",
    "            # Update action count for previous action\n",
    "            self.N_t[action] += 1\n",
    "            # Use iterative form of Q_t(a)\n",
    "            self.Q_t[action] += (1 / self.N_t[action]) * (reward_dist[action] - self.Q_t[action])\n",
    "        else:\n",
    "            # Check if epsilon is scalar or callable\n",
    "            new_epsilon = self._epsilon if np.isscalar(self._epsilon) else self._epsilon(self.t)\n",
    "            # A_t(a) is the 'action' chosen at time step 't'\n",
    "            action = np.random.choice(np.where(self.Q_t == np.max(self.Q_t))[0]) if np.random.uniform() < 1 - new_epsilon else np.random.randint(0, self.Q_t.shape[0])\n",
    "            # Update action count for previous action\n",
    "            self.N_t[action] += 1\n",
    "            # Use iterative form of Q_t(a)\n",
    "            self.Q_t[action] += (reward_dist[action] - self.Q_t[action]) / self.N_t[action]\n",
    "        # Update time step counter\n",
    "        self.t += 1\n",
    "        # Update true rewards\n",
    "        self.rewards.append(reward_dist[action])\n",
    "        # Update actions\n",
    "        self.actions.append(action)\n",
    "\n",
    "    \"\"\"\n",
    "    Reset Epsilon Greedy agent.\n",
    "    \"\"\"\n",
    "    def reset(self):\n",
    "        # Q_t(a) is the estimated value of action â€˜aâ€™ at time step â€˜tâ€™\n",
    "        self.Q_t = np.zeros(self._number_of_arms)\n",
    "        # N_t(a) is the number of times that action â€˜aâ€™ has been selected, prior to time â€˜tâ€™\n",
    "        self.N_t = np.zeros(self._number_of_arms)\n",
    "        # Set time step counter\n",
    "        self.t = 0\n",
    "        # Set true rewards\n",
    "        self.rewards = []\n",
    "        # Set actions\n",
    "        self.actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialise UCB agent. \n",
    "    - This agent returns an action between 0 and 'number_of_arms'.\n",
    "    - This agent uses uncertainty in the action-value estimates for balancing exploration and exploitation.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, number_of_arms: int, bonus_multiplier: float):\n",
    "        self._number_of_arms = number_of_arms\n",
    "        self._bonus_multiplier = bonus_multiplier\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    \"\"\"\n",
    "    Execute UCB agent's next action and update UCB's action-state values.\n",
    "    \"\"\"\n",
    "    def step(self, reward_dist: List[Union[float, int]]) -> int:\n",
    "        # All actions must be selected at least once before UCB is applied\n",
    "        if np.any(self.N_t == 0):\n",
    "            # Select non-explored action\n",
    "            action = np.random.choice(np.where(self.N_t == 0)[0])\n",
    "            # Update action count for previous action\n",
    "            self.N_t[action] += 1\n",
    "            # Use iterative form of Q_t(a)\n",
    "            self.Q_t[action] += (1 / self.N_t[action]) * (reward_dist[action] - self.Q_t[action])\n",
    "        else:\n",
    "            # Calculate expected reward values\n",
    "            reward_values = self.Q_t + self._bonus_multiplier * np.sqrt(np.log(self.t) / self.N_t)\n",
    "            # A_t(a) is the 'action' chosen at time step 't'\n",
    "            action = np.random.choice(np.where(reward_values == np.max(reward_values))[0])\n",
    "            # Update action count for previous action\n",
    "            self.N_t[action] += 1\n",
    "            # Use iterative form of Q_t(a)|\n",
    "            self.Q_t[action] += (1 / self.N_t[action]) * (reward_dist[action] - self.Q_t[action])\n",
    "        # Update time step counter\n",
    "        self.t += 1\n",
    "        # Update true rewards\n",
    "        self.rewards.append(reward_dist[action])\n",
    "        # Update actions\n",
    "        self.actions.append(action)\n",
    "\n",
    "    \"\"\"\n",
    "    Reset UCB agent.\n",
    "    \"\"\"\n",
    "    def reset(self):\n",
    "        # Q_t(a) is the estimated value of action â€˜aâ€™ at time step â€˜tâ€™\n",
    "        self.Q_t = np.zeros(self._number_of_arms)\n",
    "        # N_t(a) is the number of times that action â€˜aâ€™ has been selected, prior to time â€˜tâ€™\n",
    "        self.N_t = np.zeros(self._number_of_arms)\n",
    "        # Set time step counter\n",
    "        self.t = 0\n",
    "        # Set true rewards\n",
    "        self.rewards = []\n",
    "        # Set actions\n",
    "        self.actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Greedy Algorithm\n",
    "greedy_alg = EpsilonGreedy(name=\"Greedy\", \n",
    "                           number_of_arms=3, \n",
    "                           epsilon=0)\n",
    "# Define Epsilon-Greedy Algorithm\n",
    "eps_greedy_alg = EpsilonGreedy(name=\"Epsilon-Greedy\", \n",
    "                           number_of_arms=3, \n",
    "                           epsilon=0.4)\n",
    "# Define UCB Algorithm\n",
    "ucb_alg = UCB(name=\"UCB\", \n",
    "              number_of_arms=3, \n",
    "              bonus_multiplier=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_armed_bandits(algs: List, reward_dist: Dict[int, Union[int, float]], num_iter: int):\n",
    "    # Loop over number of iterations\n",
    "    for _ in range(num_iter):\n",
    "        # Loop over each algorithm\n",
    "        for i in range(len(algs)):\n",
    "            # Execute remaining steps of algorithm\n",
    "            algs[i].step(reward_dist=reward_dist)\n",
    "    # Plot cumulative rewards\n",
    "    plt.plot(np.cumsum(greedy_alg.rewards), label=\"Greedy\")\n",
    "    plt.plot(np.cumsum(eps_greedy_alg.rewards), label=\"Epsilon Greedy\")\n",
    "    plt.plot(np.cumsum(ucb_alg.rewards), label=\"UCB\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Cumulative Rewards\")\n",
    "    plt.show()\n",
    "    # Plot distribution of pulled arms\n",
    "    categories = [\"Arm 1\", \"Arm 2\", \"Arm 3\"]\n",
    "    bar_width = 0.2 \n",
    "    index = np.arange(len(categories)) \n",
    "    # Create bar plots for each set of values\n",
    "    plt.bar(index - bar_width, greedy_alg.Q_t, bar_width, label='Greedy')\n",
    "    plt.bar(index, eps_greedy_alg.Q_t, bar_width, label='Epsilon Greedy')\n",
    "    plt.bar(index + bar_width, ucb_alg.Q_t, bar_width, label='UCB')\n",
    "    # Customise the plot\n",
    "    plt.xlabel('Arms')\n",
    "    plt.ylabel('Reward Estimate Value')\n",
    "    plt.title(r'Approximate Reward Estimate Value Function - $Q_{t}\\left(a\\right)$')\n",
    "    plt.xticks(index, categories)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run algorithm\n",
    "kwargs = {\"algs\": [greedy_alg, eps_greedy_alg, ucb_alg],\n",
    "          \"reward_dist\": {0: np.random.uniform(low=-2, high=2), 1: 5, 2: np.random.normal(loc=10, scale=0.05)},\n",
    "          \"num_iter\": 50}\n",
    "# NOTE: Since this is in place, the algorithms will get updated dynamically!\n",
    "run_multi_armed_bandits(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What do you notice about $N_{t}\\left(a\\right)$ for the respective algorithms? Do any of them perform similarly or differently?\n",
    "2. What do you notice about $Q_{t}\\left(a\\right)$ for the respective algorithms? Do any of them perform similarly or differently?\n",
    "3. What happens if you change the $epsilon$ value in Epsilon-Greedy?\n",
    "4. What happens if you change the $multiplier$ value in UCB?\n",
    "5. What happens if you change the iteration length?\n",
    "6. What happens if you make sure that all arms are pulled, at least once first, and then continue the algorithm? Note that only UCB does this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading this notebook. Note that there are other implementations of recurrent neural networks (which I would advise you to take a look at to see any differences of similarities with this version).\n",
    "If there are any mistakes or things that need more clarity, feel free to respond and I will be happy to reply ðŸ˜Š.\n",
    "\n",
    "Â© *PolyNath 2023*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-2023",
   "language": "python",
   "name": "msc-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
