{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from functools import partial\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the setup below, we assume the following:\n",
    "\n",
    "1. We start at $t=0$ and thus there is no previous action or current reward. \n",
    "2. All arms must be pulled at least **once** before executing the entire algorithm - this is known as $warm\\_up$. \n",
    "3. Our focus is on system that returns a binary reward (for each arm) on the basis of a success or a failure through a probability measure - this is known as **multi-armed-bernoulli-bandits**.\n",
    "\n",
    "Depending on the current needs, we can adapt the code - an alternative setup is considered in the `.py` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit:\n",
    "    \n",
    "    def __init__(self, success_probabilities, success_reward=1, fail_reward=0):\n",
    "      \"\"\"\n",
    "      Constructor of a stationary Bernoulli bandit.\n",
    "      \"\"\"\n",
    "      self._probs = success_probabilities\n",
    "      self._number_of_arms = len(self._probs)\n",
    "      self._s = success_reward\n",
    "      self._f = fail_reward\n",
    "      ps = np.array(success_probabilities)\n",
    "      self._values = ps * success_reward + (1 - ps) * fail_reward\n",
    "\n",
    "    def step(self, action):\n",
    "      \"\"\"\n",
    "      The step function which takes an action and returns a reward sampled \n",
    "      according to the success probability of the selected arm.\n",
    "      \"\"\"\n",
    "      if action < 0 or action >= self._number_of_arms:\n",
    "        raise ValueError('Action {} is out of bounds for a '\n",
    "                        '{}-armed bandit'.format(action, self._number_of_arms))\n",
    "\n",
    "      success = bool(np.random.random() < self._probs[action])\n",
    "      reward = success * self._s + (not success) * self._f\n",
    "      return reward\n",
    "\n",
    "    def regret(self, action):\n",
    "      \"\"\"\n",
    "      Computes the regret for the given action.\n",
    "      \"\"\"\n",
    "      return self._values.max() - self._values[action]\n",
    "\n",
    "    def optimal_value(self):\n",
    "      \"\"\"\n",
    "      Computes the regret for the given action.\n",
    "      \"\"\"\n",
    "      return self._values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random:\n",
    "\n",
    "    def __init__(self, name, number_of_arms):\n",
    "        \"\"\"\n",
    "        A random agent.\n",
    "        This agent returns an action between 0 and 'number_of_arms', uniformly at\n",
    "        random. The 'previous_action' argument of 'step' is ignored.\n",
    "        \"\"\"\n",
    "        self._number_of_arms = number_of_arms\n",
    "        self.name = name\n",
    "\n",
    "    def step(self, unused_previous_action, unused_reward):\n",
    "        \"\"\"\n",
    "        Returns a random action.\n",
    "        The inputs are ignored, but this function still requires an action and a\n",
    "        reward, to have the same interface as other agents who may use these inputs\n",
    "        to learn.\n",
    "        \"\"\"\n",
    "        return np.random.randint(self._number_of_arms)\n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "\n",
    "    def __init__(self, name: str, number_of_arms: int, epsilon=Union[float, callable]):\n",
    "        \"\"\"\n",
    "        Initialise epsilon-greedy agent.\n",
    "        - This agent returns an action between 0 and 'number_of_arms'.\n",
    "        - It does so with probability `(1-epsilon)` it chooses the action with the highest estimated value, while\n",
    "        with probability `epsilon`, it samples an action uniformly at random.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self._number_of_arms = number_of_arms\n",
    "        self._epsilon = epsilon\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, bandit) -> int:\n",
    "        \"\"\"\n",
    "        Execute Epsilon-Greedy agent's next action and update Epsilon Greedy's action-state values.\n",
    "        \"\"\"\n",
    "        # All actions must be selected at least once before Epsilon-Greedy is applied\n",
    "        if np.any(self.N_t == 0):\n",
    "            # Select non-explored action\n",
    "            action = np.random.choice(np.where(self.N_t == 0)[0])\n",
    "            # Obtain current reward\n",
    "            reward = bandit.step(action)\n",
    "            # Update action count for previous action\n",
    "            self.N_t[action] += 1\n",
    "            # Use iterative form of Q_t(a)\n",
    "            self.Q_t[action] += (1 / self.N_t[action]) * (reward - self.Q_t[action])\n",
    "        else:\n",
    "            # Check if epsilon is scalar or callable\n",
    "            new_epsilon = self._epsilon if np.isscalar(self._epsilon) else self._epsilon(self.t)\n",
    "            # A_t(a) is the 'action' chosen at time step 't'\n",
    "            action = np.random.choice(np.where(self.Q_t == np.max(self.Q_t))[0]) if np.random.uniform() < 1 - new_epsilon else np.random.randint(0, self.Q_t.shape[0])\n",
    "            # Obtain current reward\n",
    "            reward = bandit.step(action)\n",
    "            # Update action count for previous action\n",
    "            self.N_t[action] += 1\n",
    "            # Use iterative form of Q_t(a)\n",
    "            self.Q_t[action] += (reward - self.Q_t[action]) / self.N_t[action]\n",
    "        # Calculate regret and optimal value \n",
    "        regret = bandit.regret(action)\n",
    "        optimal_value = bandit.optimal_value()\n",
    "        # Update time step counter\n",
    "        self.t += 1\n",
    "        return action, reward, regret, optimal_value\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset Epsilon Greedy agent.\n",
    "        \"\"\"\n",
    "        # Q_t(a) is the estimated value of action â€˜aâ€™ at time step â€˜tâ€™\n",
    "        self.Q_t = np.zeros(self._number_of_arms)\n",
    "        # N_t(a) is the number of times that action â€˜aâ€™ has been selected, prior to time â€˜tâ€™\n",
    "        self.N_t = np.zeros(self._number_of_arms)\n",
    "        # Set time step counter\n",
    "        self.t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    \n",
    "    def __init__(self, name: str, number_of_arms: int, bonus_multiplier: float):\n",
    "        \"\"\"\n",
    "        Initialise UCB agent. \n",
    "        - This agent returns an action between 0 and 'number_of_arms'.\n",
    "        - This agent uses uncertainty in the action-value estimates for balancing exploration and exploitation.\n",
    "        \"\"\"\n",
    "        self._number_of_arms = number_of_arms\n",
    "        self._bonus_multiplier = bonus_multiplier\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, bandit) -> int:\n",
    "        \"\"\"\n",
    "        Execute UCB agent's next action and update UCB's action-state values.\n",
    "        \"\"\"\n",
    "        # All actions must be selected at least once before UCB is applied\n",
    "        if np.any(self.N_t == 0):\n",
    "            # Select non-explored action\n",
    "            action = np.random.choice(np.where(self.N_t == 0)[0])\n",
    "            # Obtain current reward\n",
    "            reward = bandit.step(action)\n",
    "            # Update action count for previous action\n",
    "            self.N_t[action] += 1\n",
    "            # Use iterative form of Q_t(a)\n",
    "            self.Q_t[action] += (1 / self.N_t[action]) * (reward - self.Q_t[action])\n",
    "        else:\n",
    "            # Calculate expected reward values\n",
    "            reward_values = self.Q_t + self._bonus_multiplier * np.sqrt(np.log(self.t) / self.N_t)\n",
    "            # A_t(a) is the 'action' chosen at time step 't'\n",
    "            action = np.random.choice(np.where(reward_values == np.max(reward_values))[0])\n",
    "            # Obtain current reward\n",
    "            reward = bandit.step(action)\n",
    "            # Update action count for previous action\n",
    "            self.N_t[action] += 1\n",
    "            # Use iterative form of Q_t(a)|\n",
    "            self.Q_t[action] += (1 / self.N_t[action]) * (reward - self.Q_t[action])\n",
    "        # Calculate regret and optimal value \n",
    "        regret = bandit.regret(action)\n",
    "        optimal_value = bandit.optimal_value()\n",
    "        # Update time step counter\n",
    "        self.t += 1\n",
    "        return action, reward, regret, optimal_value\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset UCB agent.\n",
    "        \"\"\"\n",
    "        # Q_t(a) is the estimated value of action â€˜aâ€™ at time step â€˜tâ€™\n",
    "        self.Q_t = np.zeros(self._number_of_arms)\n",
    "        # N_t(a) is the number of times that action â€˜aâ€™ has been selected, prior to time â€˜tâ€™\n",
    "        self.N_t = np.zeros(self._number_of_arms)\n",
    "        # Set time step counter\n",
    "        self.t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments repeatedly\n",
    "def run_experiments(bandit_constructor, algs, repetitions, number_of_steps):\n",
    "  # Store actions\n",
    "  action_dict = {}\n",
    "  # Store rewards\n",
    "  reward_dict = {}\n",
    "  # Store regret\n",
    "  regret_dict = {}\n",
    "  # Store optimal value\n",
    "  optimal_value_dict = {}\n",
    "  # Loop through algorithms\n",
    "  for alg in algs:\n",
    "    # For each algorithm, create dictionary storing rewards, regret and optimal values\n",
    "    action_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
    "    reward_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
    "    regret_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
    "    optimal_value_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
    "    # Loop through number of repetitions\n",
    "    for _rep in range(repetitions):\n",
    "      # Create Bernoulli Bandit simulation\n",
    "      bandit = bandit_constructor()\n",
    "      # Reset algorithm\n",
    "      alg.reset()\n",
    "      # Execute Multi-Armed Simulation\n",
    "      for _step in range(number_of_steps):\n",
    "        # Obtain action, reward, regret and optimal value at current time step\n",
    "        action, reward, regret, optimal_value = alg.step(bandit)\n",
    "        # Store results\n",
    "        action_dict[alg.name][_rep, _step] = action\n",
    "        reward_dict[alg.name][_rep, _step] = reward\n",
    "        regret_dict[alg.name][_rep, _step] = regret\n",
    "        optimal_value_dict[alg.name][_rep, _step] = optimal_value\n",
    "  return action_dict, reward_dict, regret_dict, optimal_value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def smooth(array, smoothing_horizon=100., initial_value=0.):\n",
    "    \"\"\"Smoothing function for plotting.\"\"\"\n",
    "    smoothed_array = []\n",
    "    value = initial_value\n",
    "    b = 1./smoothing_horizon\n",
    "    m = 1.\n",
    "    for x in array:\n",
    "        m *= 1. - b\n",
    "        lr = b/(1 - m)\n",
    "        value += lr*(x - value)\n",
    "        smoothed_array.append(value)\n",
    "    return np.array(smoothed_array)\n",
    "def calculate_lims(data, log_plot=False):\n",
    "    \"\"\"Calculating limits.\"\"\"\n",
    "    y_min = np.min(data)\n",
    "    y_max = np.max(data)\n",
    "    diff = y_max - y_min\n",
    "    if log_plot:\n",
    "        y_min = 0.9*y_min\n",
    "        y_max = 1.1*y_max\n",
    "    else:\n",
    "        y_min = y_min - 0.05*diff\n",
    "        y_max = y_max + 0.05*diff\n",
    "    return y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot experiments\n",
    "def plot(algs, plot_data, repetitions=30):\n",
    "    algs_per_row = 4\n",
    "    n_algs = len(algs)\n",
    "    n_rows = (n_algs - 2)//algs_per_row + 1\n",
    "    fig = plt.figure(figsize=(10, 4*n_rows))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace=0.35)\n",
    "    clrs = ['#000000', '#00bb88', '#0033ff', '#aa3399', '#ff6600']\n",
    "    lss = ['--', '-', '-', '-', '-']\n",
    "    for i, p in enumerate(plot_data):\n",
    "      for c in range(n_rows):\n",
    "        ax = fig.add_subplot(n_rows, len(plot_data), i + 1 + c*len(plot_data))\n",
    "        ax.grid(0)\n",
    "        current_algs = [algs[0]] + algs[c*algs_per_row + 1:(c + 1)*algs_per_row + 1]\n",
    "        for alg, clr, ls in zip(current_algs, clrs, lss):\n",
    "          data = p.data[alg.name]\n",
    "          m = smooth(np.mean(data, axis=0))\n",
    "          s = np.std(smooth(data.T).T, axis=0)/np.sqrt(repetitions)\n",
    "          if p.log_plot:\n",
    "            line = plt.semilogy(m, alpha=0.7, label=alg.name,\n",
    "                                color=clr, ls=ls, lw=3)[0]\n",
    "          else:\n",
    "            line = plt.plot(m, alpha=0.7, label=alg.name,\n",
    "                            color=clr, ls=ls, lw=3)[0]\n",
    "            plt.fill_between(range(len(m)), m + s, m - s,\n",
    "                            color=line.get_color(), alpha=0.2)\n",
    "        if p.opt_values is not None:\n",
    "          plt.plot(p.opt_values[current_algs[0].name][0], ':', alpha=0.5,\n",
    "                  label='optimal')\n",
    "\n",
    "        ax.set_facecolor('white')\n",
    "        ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",\n",
    "                      labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set(visible=True, color='black', lw=1)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set(visible=True, color='black', lw=1)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "\n",
    "        data = np.array([smooth(np.mean(d, axis=0)) for d in p.data.values()])\n",
    "        \n",
    "        if p.log_plot:\n",
    "          start, end = calculate_lims(data, p.log_plot)\n",
    "          start = np.floor(np.log10(start))\n",
    "          end = np.ceil(np.log10(end))\n",
    "          ticks = [_*10**__\n",
    "                  for _ in [1., 2., 3., 5.]\n",
    "                  for __ in [-2., -1., 0.]]\n",
    "          labels = [r'${:1.2f}$'.format(_*10** __)\n",
    "                    for _ in [1, 2, 3, 5]\n",
    "                    for __ in [-2, -1, 0]]\n",
    "          plt.yticks(ticks, labels)\n",
    "        plt.ylim(calculate_lims(data, p.log_plot))\n",
    "        plt.locator_params(axis='x', nbins=4)\n",
    "        \n",
    "        plt.title(p.title)\n",
    "        if i == len(plot_data) - 1:\n",
    "          plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Reinforcement Learning Algorithsms\n",
    "def train_agents(agents, number_of_arms, \n",
    "                 number_of_steps, repetitions=100,\n",
    "                 success_reward=1, fail_reward=0,\n",
    "                 bandit_class=BernoulliBandit):\n",
    "        # Note succcess probabilities of pulling an arm\n",
    "        success_probabilities = np.arange(0.3, 0.7 + 1e-6, 0.4/(number_of_arms - 1))\n",
    "        # Create Bernoulli Bandit construct\n",
    "        bandit_constructor = partial(bandit_class,\n",
    "                                     success_probabilities=success_probabilities,\n",
    "                                     success_reward=success_reward,\n",
    "                                     fail_reward=fail_reward)\n",
    "        # Run experiment\n",
    "        actions, rewards, regrets, opt_values = run_experiments(bandit_constructor, agents, repetitions, number_of_steps)\n",
    "        # Plot experiments\n",
    "        smoothed_rewards = {}\n",
    "        for agent, rs in rewards.items():\n",
    "            smoothed_rewards[agent] = np.array(rs)\n",
    "        PlotData = collections.namedtuple('PlotData',\n",
    "                                            ['title', 'data', 'opt_values', 'log_plot'])\n",
    "        total_regrets = dict([(k, np.cumsum(v, axis=1)) for k, v in regrets.items()])\n",
    "        plot_data = [\n",
    "            PlotData(title='Smoothed rewards', data=smoothed_rewards,\n",
    "                    opt_values=opt_values, log_plot=False),\n",
    "            PlotData(title='Current Regret', data=regrets, opt_values=None,\n",
    "                    log_plot=True),\n",
    "            PlotData(title='Total Regret', data=total_regrets, opt_values=None,\n",
    "                    log_plot=False),\n",
    "            PlotData(title='Actions', data=actions, opt_values=None,\n",
    "                    log_plot=False),\n",
    "        ]\n",
    "        plot(agents, plot_data, repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Bandit - 5 arms (reward=1 on success, and a reward=0 on failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture experiment1\n",
    "number_of_arms = 5\n",
    "number_of_steps = 500\n",
    "agents = [Random(name=\"Random\",\n",
    "                 number_of_arms=number_of_arms),\n",
    "          EpsilonGreedy(name=r\"Greedy ($\\epsilon=0$)\", \n",
    "                        number_of_arms=number_of_arms, \n",
    "                        epsilon=0),\n",
    "          EpsilonGreedy(name=r\"Epsilon-Greedy ($\\epsilon=\\frac{1}{\\sqrt{t}}$)\", \n",
    "                        number_of_arms=number_of_arms, \n",
    "                        epsilon=lambda t: 1/np.sqrt(t)),\n",
    "          EpsilonGreedy(name=r\"Epsilon-Greedy ($\\epsilon=\\frac{1}{t}$)\", \n",
    "                        number_of_arms=number_of_arms, \n",
    "                        epsilon=lambda t: 1/t),\n",
    "          UCB(name=r\"UCB ($c=2$)\", \n",
    "              number_of_arms=number_of_arms, \n",
    "              bonus_multiplier=2)]\n",
    "train_agents(agents, number_of_arms, number_of_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots\n",
    "experiment1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What do you notice about $N_{t}\\left(a\\right)$ for the respective algorithms? Do any of them look similar or different?\n",
    "2. What do you notice about $Q_{t}\\left(a\\right)$ for the respective algorithms? Do any of them look similar or different?\n",
    "3. What happens if you change the $number\\_of\\_arms$?\n",
    "4. What happens if you change the $epsilon$ value in Epsilon-Greedy?\n",
    "5. What happens if you change the $multiplier$ value in UCB?\n",
    "6. What happens if you change the $num\\_iter$?\n",
    "7. What happens if you make sure that it does not matter if all arms are pulled at least once first?\n",
    "8. What happens if you change the $reward\\_dist$?\n",
    "9. What happens if the distribution of rewards became non-stationary i.e. changed after every pull?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading this notebook. Note that there are other implementations of recurrent neural networks (which I would advise you to take a look at to see any differences of similarities with this version).\n",
    "If there are any mistakes or things that need more clarity, feel free to respond and I will be happy to reply ðŸ˜Š.\n",
    "\n",
    "Â© *PolyNath 2023*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-2023",
   "language": "python",
   "name": "msc-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
